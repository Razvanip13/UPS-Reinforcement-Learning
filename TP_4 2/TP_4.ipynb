{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4 : Approximation Functions & Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extend the methods we have seen for discrete state spaces to problems with arbitrarily large state spaces. In the case of a video game for example, the number of possible images is often larger than the number of atoms in the observable universe. We generally can't expect to build a Q-table.  \n",
    "\n",
    "\n",
    "Our goal instead is to find a good approximate solution using limited computational resources.  Because we will encounter states never seen before, we need to use data from previous encounter to perform generalization. We will see function approximation as a generalization method. Function approximation is a supervised learning problem, as such we will be using methods you have seen in Machine Learning courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The most important parts (with almost all the points in the implementation and analysis ) are for MC, TD0 and n-step TD algorithm.  Deep Q-learning is added for you to experiment. Stuff might break. It might take a long time to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please send this practical with plots before next course to cyriaque.rousselot@inria.fr If your file is too big due to videos, share a link to the videos folder instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you are asked to solve the classic Mountain Car Problem (https://gymnasium.farama.org/environments/classic_control/mountain_car/#mountain-car). Unlike previous environment, states are continuous so that you need to approximate the Q values Q(s, a). The goal is to push the cart to the flag. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ChessUrl](https://gymnasium.farama.org/_images/mountain_car.gif \"Frozen Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages required\n",
    "# pip install moviepy scipy opencv-python gymnasium[atari,accept-rom-license,classic-control] torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the impact of several action\n",
    "env = utils.WithSnapshots(gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"))\n",
    "state, _ = env.reset()\n",
    "n_cols = 5\n",
    "n_rows = 2\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n",
    "        ax.imshow(env.render())\n",
    "        action = env.action_space.sample()\n",
    "        ax.set_title(f\"Action is {action}\")\n",
    "        env.step(action)\n",
    "\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, _, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    if terminated:\n",
    "        print(\"Whoops! We died!\")\n",
    "        break\n",
    "    if truncated:\n",
    "        print(\"Time is over!\")\n",
    "        break\n",
    "\n",
    "print(\"final state:\")\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose(self, state):\n",
    "        return env.action_space.sample()\n",
    "\n",
    "\n",
    "def generate_session(env, agent):\n",
    "    \"\"\"play env with agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = agent.choose(s)\n",
    "        observation, reward, terminated, truncated, info = env.step(a)\n",
    "        total_reward += reward\n",
    "        s = observation\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output a video (not mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", max_episode_steps=1000)\n",
    "env = RecordVideo(env, video_folder=\"videos\")\n",
    "env = utils.WithSnapshots(env)\n",
    "generate_session(\n",
    "    env,\n",
    "    Agent(),\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the snapshot to suggest effect of an action, then load back the snapshot\n",
    "def suggest_model(action, env):\n",
    "    snap0 = env.get_snapshot()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.load_snapshot(snap0)\n",
    "    return (observation, reward, terminated, truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomAgentQ:\n",
    "    \"\"\"Dummy Agent\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_env(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def preprocessing(self, state):\n",
    "        \"\"\"\n",
    "        Return the featurized representation of a state.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Output optimal action of a given state\n",
    "        Return: action in [0, 1, 2]\n",
    "        \"\"\"\n",
    "        return np.random.choice([0, 1, 2])\n",
    "\n",
    "    def update(self, s1, a1, r1, s2):\n",
    "        pass\n",
    "\n",
    "    def q(self, state, action):\n",
    "        \"\"\"Final Q function.\n",
    "        Value (scalar): Q(state, action)\n",
    "        \"\"\"\n",
    "        return np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_train(agent, n_episodes=3, max_iter=10, verbose=True):\n",
    "    env = gym.make(\n",
    "        \"MountainCar-v0\", render_mode=\"rgb_array\", max_episode_steps=max_iter\n",
    "    )\n",
    "    # env = RecordVideo(env, video_folder=\"videos\")\n",
    "    env = utils.WithSnapshots(env)\n",
    "    agent.add_env(env)\n",
    "    cumul_reward = 0\n",
    "    list_n_episodes = []\n",
    "    for g in range(n_episodes):\n",
    "        print(f\"Episode {g+1}\")\n",
    "        state, _ = env.reset()\n",
    "        stop = False\n",
    "        i = 0\n",
    "        s_reward = 0\n",
    "        while not stop:\n",
    "            if verbose:\n",
    "                print(\"Simulation step {}:\".format(i))\n",
    "            action = agent.act(state)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            stop = terminated or truncated\n",
    "            agent.update(state, action, reward, observation)\n",
    "\n",
    "            cumul_reward += reward\n",
    "            s_reward += reward\n",
    "            if verbose:\n",
    "                print(\" ->       game: {}\".format(g))\n",
    "                print(\" ->       observation: {}\".format(state))\n",
    "                print(\" ->            action: {}\".format(action))\n",
    "                print(\" ->            reward: {}\".format(reward))\n",
    "                print(\" -> cumulative reward: {}\".format(s_reward))\n",
    "                if stop:\n",
    "                    print(\" ->    Terminal event: {}\".format(observation))\n",
    "                    input()\n",
    "                print(\"\")\n",
    "            i += 1\n",
    "            state = observation\n",
    "        list_n_episodes.append(s_reward)\n",
    "        if verbose:\n",
    "            print(\" <=> Finished episode number: {} <=>\".format(g + 1))\n",
    "            print(\"\")\n",
    "    return cumul_reward, list_n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgentQ()\n",
    "cumul_reward, list_n_episodes = loop_train(agent, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q.0 Build a loop to evaluate your agent on a new episode. you can use this next function for visualisation of the Q or V function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_to_go_mountain_car(env, type_value, estimator, num_tiles=20):\n",
    "    x = np.linspace(\n",
    "        env.observation_space.low[0], env.observation_space.high[0], num=num_tiles\n",
    "    )\n",
    "    y = np.linspace(\n",
    "        env.observation_space.low[1], env.observation_space.high[1], num=num_tiles\n",
    "    )\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    if type_value == \"state_value\":\n",
    "        Z = np.apply_along_axis(lambda _: estimator(_), 2, np.dstack([X, Y]))\n",
    "        ax.set_title(\"State-value function: V(s)\")\n",
    "    else:\n",
    "        Z = np.apply_along_axis(\n",
    "            lambda _: np.max([estimator(_, a) for a in range(0, 1, 2)]),\n",
    "            2,\n",
    "            np.dstack([X, Y]),\n",
    "        )\n",
    "        ax.set_title(\"State-action value Function: $max_{a}q(s, a)$\")\n",
    "\n",
    "    surf = ax.plot_surface(\n",
    "        X, Y, Z, rstride=1, cstride=1\n",
    "    )  # , cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0\n",
    "    ax.set_xlabel(\"Position\")\n",
    "    ax.set_ylabel(\"Velocity\")\n",
    "    ax.set_zlabel(\"Value\")\n",
    "    fig.colorbar(surf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_to_go_mountain_car(env, \"q_value\", agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear approximation of state Value $V(s)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first are going to consider an parametrized estimation of the value function $$v_\\pi(s) \\sim \\hat{v}(s,\\bold{\\theta})$$ with parameters $\\bold{\\theta}$. You can think of the process of generalization in two different ways :\n",
    "- We simplify the problem by computing similarity elements between states  -> feature learning\n",
    "- We extend the behavior of the policy to multiple states -> Optimization on a parametrized function class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we introduce a mixture of gaussians that represent each state using `RBFSampler` from scikit-learn. \n",
    "Each state observation becomes an array of similarity between the current state point and a fix basis of point. The similarity is computed using a Gaussian kernel \n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/kernel_approximation.html#rbf-kernel-approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo State Value Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q.1 Complete and Analyze the behavior of the Linear Monte-Carlo State Value Approximation algorithm ( See Sutton Barto, 9.3 and 9.4) for multiple number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_State_Value_Approximation:\n",
    "    \"\"\"MC State-value Learning with Function Approximation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init:\n",
    "            gamma: discount factor\n",
    "            preprocessing_states: preprocessing state features for better representation\n",
    "\n",
    "            theta: linear weight (function approximation)\n",
    "        \"\"\"\n",
    "        self.gamma = 0.9\n",
    "        n_components = 20  # You can change\n",
    "        self.preprocessing_states = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"feature_generation\", RBFSampler(n_components=n_components)),\n",
    "            ]\n",
    "        )\n",
    "        self.preprocessing_states.fit(\n",
    "            np.array(\n",
    "                [\n",
    "                    [np.random.uniform(-1.2, 0.6), np.random.uniform(-0.07, 0.07)]\n",
    "                    for _ in range(10000)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.theta = np.zeros(n_components)  # Linear weights\n",
    "\n",
    "    def preprocessing(self, state):\n",
    "        \"\"\"\n",
    "        Return the featurized representation of a state.\n",
    "        \"\"\"\n",
    "        return self.preprocessing_states.transform([state])[0]\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Output optimal action of a given state\n",
    "        Returns: action in [0, 1, 2]\n",
    "        \"\"\"\n",
    "\n",
    "        # TO COMPLETE\n",
    "        idx_best = None\n",
    "\n",
    "        if np.random.uniform(0, 1) < 0.1:\n",
    "            return np.random.choice(list(idx_best[1:]))\n",
    "        else:\n",
    "            return idx_best[0]\n",
    "\n",
    "    def update(self, state, action, reward, new_state, terminal):\n",
    "        # TO COMPLETE\n",
    "        self.theta = self.theta + 1e-3 * (target - self.v(state)) * self.preprocessing(\n",
    "            state\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def v(self, state):\n",
    "        \"\"\"Final V function.\n",
    "        Return:\n",
    "            Value (scalar): V(state)\n",
    "        \"\"\"\n",
    "        return np.dot(self.preprocessing(state), self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q.2 Implement the Linear Semi-Gradient TD(0) : ( See Sutton Barto, 9.3 and 9.4). Analyze its behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q.3 Implement the n-step-TD State Value Approximation.  Analyze its behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Q Learning with Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  ! experimental\n",
    "\n",
    "> Fill the code bellow and analyse your training process, the behavior of the agent and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a Deep-Q network  https://arxiv.org/pdf/1312.5602.pdf on the break-out Atari game. Instead of a fixed basis to decompose the state, we will learn a representation of each state using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ChessUrl](https://gymnasium.farama.org/_images/breakout.gif \"Frozen Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "\n",
    "\n",
    "# Play the game yourself\n",
    "play(env=gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\"), zoom=4, fps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Processing game image\n",
    "\n",
    "Raw Atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn from them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "\n",
    "    Resizing to a smaller shape, 64x64\n",
    "    Gray-scale the image\n",
    "    Cropping irrelevant image parts (top, bottom and edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import GrayScaleObservation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def observation(state):\n",
    "    img = Image.fromarray(state)\n",
    "    img = img.crop((8, 32, 152, 195))\n",
    "    img = img.resize((64, 64))\n",
    "    img = np.array(img, dtype=np.float32) / 255\n",
    "    return img\n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "env = GrayScaleObservation(env)\n",
    "state, _ = env.reset()\n",
    "obs = observation(state)\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"before cropping\")\n",
    "plt.imshow(state)\n",
    "plt.show()\n",
    "print(\"after cropping\")\n",
    "plt.imshow(obs,cmap=\"gist_gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Frame buffer\n",
    "\n",
    "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so it must be able to measure object's velocity. To do so, we introduce a buffer that stores 4 last images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "env = GrayScaleObservation(env)\n",
    "env = TransformObservation(env, lambda obs: observation(obs))\n",
    "env = ResizeObservation(env, 64)\n",
    "env = FrameStack(env, 4)\n",
    "state, _ = env.reset()\n",
    "\n",
    "plt.imshow(state[0], cmap=\"gist_gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
    "\n",
    "You can build any architecture you want, but for reference, here's something that will more or less work:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](dqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using pytorch https://pytorch.org/docs/stable/index.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        # nn.Flatten() can be useful\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = \n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your agent on this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
    "    \"\"\"Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset(seed=seed)\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = (\n",
    "                qvalues.argmax(axis=-1)[0]\n",
    "                if greedy\n",
    "                else agent.sample_actions(qvalues)[0]\n",
    "            )\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = (4, 64, 64)\n",
    "n_actions = env.action_space.n\n",
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we provide you with experience replay buffer. This constitute your \"training set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](exp_replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(), 1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
    "\n",
    "assert (\n",
    "    len(exp_replay) == 10\n",
    "), \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    <YOUR CODE>\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state,_ = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, (\n",
    "    \"play_and_record should have added exactly 1000 steps, \"\n",
    "    \"but instead added %i\" % len(exp_replay)\n",
    ")\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1, (\n",
    "    \"Please make sure you restart the game whenever it is 'done' and \"\n",
    "    \"record the is_done correctly into the buffer. Got %f is_done rate over \"\n",
    "    \"%i steps. [If you think it's your tough luck, just re-run the test]\"\n",
    "    % (np.mean(is_dones), len(exp_replay))\n",
    ")\n",
    "\n",
    "for _ in range(100):\n",
    "    (\n",
    "        obs_batch,\n",
    "        act_batch,\n",
    "        reward_batch,\n",
    "        next_obs_batch,\n",
    "        is_done_batch,\n",
    "    ) = exp_replay.sample(10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,\n",
    "    ), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,\n",
    "    ), \"rewards batch should have shape (10,) but is instead %s\" % str(\n",
    "        reward_batch.shape\n",
    "    )\n",
    "    assert is_done_batch.shape == (\n",
    "        10,\n",
    "    ), \"is_done batch should have shape (10,) but is instead %s\" % str(\n",
    "        is_done_batch.shape\n",
    "    )\n",
    "    assert [\n",
    "        int(i) in (0, 1) for i in is_dones\n",
    "    ], \"is_done should be strictly True or False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch\n",
    "    ], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = <YOUR CODE>\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10\n",
    ")\n",
    "\n",
    "loss = compute_td_loss(\n",
    "    obs_batch,\n",
    "    act_batch,\n",
    "    reward_batch,\n",
    "    next_obs_batch,\n",
    "    is_done_batch,\n",
    "    agent,\n",
    "    target_network,\n",
    "    gamma=0.99,\n",
    "    check_shapes=True,\n",
    ")\n",
    "loss.backward()\n",
    "\n",
    "assert (\n",
    "    loss.requires_grad and tuple(loss.data.size()) == ()\n",
    "), \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(\n",
    "    next(agent.parameters()).grad.data.cpu().numpy() != 0\n",
    "), \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(\n",
    "    next(target_network.parameters()).grad is None\n",
    "), \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "\n",
    "**If deadline is tonight and it has not converged:** It is ok. Send the notebook today and when it converges send it again.\n",
    "If the code is exactly the same points will not be discounted.\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "env = GrayScaleObservation(env)\n",
    "env = TransformObservation(env, lambda obs: observation(obs))\n",
    "env = ResizeObservation(env, 64)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state,_ = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_shape, n_actions).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer of size $10^4$ fits into 5 Gb RAM.\n",
    "\n",
    "Larger sizes ($10^5$ and $10^6$ are common) can be used. It can improve the learning, but $10^4$ is quite enough. $10^2$ will probably fail learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "N_STEPS = 100\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in trange(REPLAY_BUFFER_SIZE // N_STEPS):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=N_STEPS)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 16\n",
    "total_steps = 3 * 10**6\n",
    "decay_steps = 10**6\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 50\n",
    "refresh_target_network_freq = 5000\n",
    "eval_freq = 5000\n",
    "\n",
    "max_grad_norm = 50\n",
    "\n",
    "n_lives = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state,_ = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        <YOUR CODE: sample batch_size of data from experience replay>\n",
    "\n",
    "        loss = <YOUR CODE: compute TD loss>\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm.cpu())\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            <YOUR CODE>\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(clip_rewards=True, seed=step), agent, n_games=3 * n_lives, greedy=True)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per life\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent is evaluated for 1 life, not for a whole episode of 5 lives. Rewards in evaluation are also truncated. Cuz this is what environment the agent is learning in and in this way mean rewards per life can be compared with initial state value\n",
    "\n",
    "**The goal is to get 15 points in the real env**. So 3 or better 4 points in the preprocessed one will probably be enough. You can interrupt learning then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final scoring is done on a whole episode with all 5 lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "    env,\n",
    "    agent,\n",
    "    n_games=30,\n",
    "    greedy=True,\n",
    "    t_max=10 * 1000,\n",
    ")\n",
    "print(\"final score:\", final_score)\n",
    "assert final_score >= 3, \"not as cool as DQN can\"\n",
    "print(\"Cool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret plots:\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* **TD loss** is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* **grad norm** just shows the intensivity of training. Not ok is growing to values of about 100 (or maybe even 50) though it depends on network architecture.\n",
    "* **mean reward** is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes about 40k steps to \"warm up\" agent before it starts to get better.\n",
    "* **Initial state V** is the expected discounted reward for episode in the oppinion of the agent. It should behave more smoothly than **mean reward**. It should get higher over time but sometimes can experience drawdowns because of the agaent's overestimates.\n",
    "* **buffer size** - this one is simple. It should go up and cap at max size.\n",
    "* **epsilon** - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - it means you need to increase epsilon. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Smoothing of plots is done with a gaussian kernel\n",
    "\n",
    "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
    "\n",
    "**Training will take time.** A lot of it actually. Probably you will not see any improvment during first **150k** time steps (note that by default in this notebook agent is evaluated every 5000 time steps).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About hyperparameters:\n",
    "\n",
    "The task has something in common with supervised learning: loss is optimized through the buffer (instead of Train dataset). But the distribution of states and actions in the buffer **is not stationary** and depends on the policy that generated it. It can even happen that the mean TD error across the buffer is very low but the performance is extremely poor (imagine the agent collecting data to the buffer always manages to avoid the ball).\n",
    "\n",
    "* Total timesteps and training time: It seems to be so huge, but actually it is normal for RL.\n",
    "\n",
    "* $\\epsilon$ decay shedule was taken from the original paper and is like traditional for epsilon-greedy policies. At the beginning of the training the agent's greedy policy is poor so many random actions should be taken.\n",
    "\n",
    "* Optimizer: In the original paper RMSProp was used (they did not have Adam in 2013) and it can work not worse than Adam. For us Adam was default and it worked.\n",
    "\n",
    "* lr: $10^{-3}$ would probably be too huge\n",
    "\n",
    "* batch size: This one can be very important: if it is too small the agent can fail to learn. Huge batch takes more time to process. If batch of size 8 can not be processed on the hardware you use take 2 (or even 4) batches of size 4, divide the loss on them by 2 (or 4) and make optimization step after both backward() calls in torch.\n",
    "\n",
    "* target network update frequency: has something in common with learning rate. Too frequent updates can lead to divergence. Too rare can lead to slow leraning. For millions of total timesteps thousands of inner steps seem ok. One iteration of target network updating is an iteration of the (this time approximate) $\\gamma$-compression that stands behind Q-learning. The more inner steps it makes the more accurate is the compression.\n",
    "* max_grad_norm - just huge enough. In torch clip_grad_norm also evaluates the norm before clipping and it can be convenient for logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parts of this practical have been taken or adapted from https://github.com/yandexdataschool/Practical_RL/_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
